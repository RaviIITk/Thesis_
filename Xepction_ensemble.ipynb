{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong path: Image data/e0_Normal/test1.tif.gz\n",
      "Wrong path: Image data/e0_Normal/test.tif.gz\n",
      "Wrong path: Image data/e0_Normal/test2.gif.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Concatenate\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "data_dir = pathlib.Path('./Image data')\n",
    "images=list(data_dir.glob('*/*.tif'))\n",
    "image_dic= {\n",
    "    'normal' : list(data_dir.glob(\"e0_Normal/*\")),\n",
    "    'hole':list(data_dir.glob(\"e1_hole/*\")),\n",
    "    'stain' : list(data_dir.glob(\"e2_Stain/*\")),\n",
    "    'net' :list(data_dir.glob(\"e3_Net/*\")),\n",
    "    'color' :list(data_dir.glob(\"e4_colour/*\")),\n",
    "    'crease': list(data_dir.glob(\"e5_Crease/*\")),\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'normal':0,\n",
    "    'hole':1,\n",
    "    'stain':2,\n",
    "    'net':3,\n",
    "    'color':4,\n",
    "    'crease':5,\n",
    "}\n",
    "X,y=[],[]\n",
    "\n",
    "for name, images in image_dic.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(str(image))\n",
    "        \n",
    "        \n",
    "        if img is None:\n",
    "            print('Wrong path:', image)\n",
    "        else:\n",
    "            resized = cv2.resize(img,IMG_SIZE)\n",
    "            X.append(resized)\n",
    "            y.append(labels[name])\n",
    "X = np.array(X)/255\n",
    "y= np.array(y)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,random_state=0, shuffle=True,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception_block(input_shape, classes,\n",
    "                   activation='sigmoid',\n",
    "                   flows=7, \n",
    "                   ensemble='S16,3,2_R_D_S32,3,2_R_D',\n",
    "                   loss='binary_crossentropy',\n",
    "                   learning_rate=0.001,\n",
    "                   weights='imagenet'):\n",
    "\n",
    "    '''\n",
    "    input_shape : input shape of image\n",
    "    classes     : number of classes \n",
    "    breakindex  : number of xception block-flows to be used \n",
    "    ensemble    : the default ensemble model to be followed for each class\n",
    "    '''\n",
    "\n",
    "    assert flows > 0 and flows < 10, \"The number of flows should be between [1-9]\"\n",
    "    assert loss in ['binary_crossentropy', 'categorical_crossentropy'], \"loss can be either 'binary_crossentropy' or 'categorical_crossentropy'\"\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    breakindex = [36, 46, 56, 66, 76, 86, 96, 106, 116]\n",
    "    \n",
    "    input_img = tf.keras.Input(input_shape, name='input')\n",
    "    base_model = tf.keras.applications.Xception(input_tensor=input_img, \n",
    "                                                include_top=False, \n",
    "                                                weights=weights)\n",
    "    \n",
    "    totLayers = len(base_model.layers)\n",
    "    for i in range(totLayers):\n",
    "        base_model.layers[i]._name += '_head'\n",
    "\n",
    "    # Last index of the head CNN\n",
    "    comb_out = base_model.layers[breakindex[flows-1]].output\n",
    "    #base_model.layers[breakindex[flows-1]]._name += '_lastcom'\n",
    "\n",
    "    seps = []\n",
    "    if ensemble != '':\n",
    "        esets = ensemble.split('_')\n",
    "    else:\n",
    "        esets = None\n",
    "\n",
    "def parser(tok, id, cls):\n",
    "    ''' \n",
    "    SepConv: S,f,k,s\n",
    "    MaxPool: P,s\n",
    "    ReLU   : R\n",
    "    Dropout: D\n",
    "    '''\n",
    "    if tok[0] == 'S':\n",
    "        filters, ks, stride = list(map(int, tok[1:].split(',')))\n",
    "        return SeparableConv2D(filters=filters, kernel_size=ks, strides=stride, \n",
    "                                padding='same', name=f'sepconv2d_d{id}_c{cls}')\n",
    "    elif tok[0] == 'P':\n",
    "        return MaxPool2D(pool_size=int(tok[1:]), name=f'MaxPool2D_d{id}_c{cls}')\n",
    "    elif tok[0] == 'R':\n",
    "        return ReLU(name=f'ReLU_d{id}_c{cls}')\n",
    "    elif tok[0] == 'D':\n",
    "        return Dropout(0.1, name=f'dropout_d{id}_c{cls}')\n",
    "    elif tok[0] == 'N':\n",
    "        return BatchNormalization()\n",
    "    else:\n",
    "        print('Invalid')\n",
    "\n",
    "\n",
    "# How to define ensemble structure:\n",
    "# S: SeparableConv2D, follows by filter, kernel, and stride \n",
    "#    example: S64,3,2\n",
    "# P: MaxPool, followed by pool size\n",
    "#    example: P2\n",
    "# R: ReLU activation\n",
    "# D: Dropout, defaults to 0.1 drop\n",
    "#\n",
    "# A full example: 'S8,3,2_R_D_S8,3,2_R_D'\n",
    "# The network is as follows:\n",
    "#    SeparableConv2D(filters=8, kernel_size=3, strides=2)\n",
    "#    ReLU()\n",
    "#    Dropout(0.1)\n",
    "#    SeparableConv2D(filters=8, kernel_size=3, strides=2)\n",
    "#    ReLU()\n",
    "#    Dropout(0.1)\n",
    "\n",
    "\n",
    "ensemble_structure = ['S64,3,2_R_D_S64,3,2_R_D',\n",
    "                      'S32,3,2_R_D_S64,3,2_R_D',\n",
    "                      'S16,3,2_R_D_S32,3,2_R_D', \n",
    "                      'S8,3,2_R_D_S8,3,2_R_D',\n",
    "                      'S4,3,2_S16,3,2']\n",
    "\n",
    "\n",
    "def Xception_block(input_shape, classes,\n",
    "                   activation='sigmoid',\n",
    "                   flows=7, \n",
    "                   \n",
    "                   loss='binary_crossentropy',\n",
    "                   learning_rate=0.001,\n",
    "                   weights='imagenet'):\n",
    "\n",
    "    '''\n",
    "    input_shape : input shape of image\n",
    "    classes     : number of classes \n",
    "    breakindex  : number of xception block-flows to be used \n",
    "    ensemble    : the default ensemble model to be followed for each class\n",
    "    '''\n",
    "\n",
    "    assert flows > 0 and flows < 10, \"The number of flows should be between [1-9]\"\n",
    "    assert loss in ['binary_crossentropy', 'categorical_crossentropy'], \"loss can be either 'binary_crossentropy' or 'categorical_crossentropy'\"\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    breakindex = [6,16,26,36, 46, 56, 66, 76, 86, 96, 106, 116]\n",
    "    \n",
    "    input_img = tf.keras.Input(input_shape, name='input')\n",
    "    base_model = tf.keras.applications.Xception(input_tensor=input_img, \n",
    "                                                include_top=False, \n",
    "                                                weights=weights)\n",
    "    \n",
    "    totLayers = len(base_model.layers)\n",
    "    for i in range(totLayers):\n",
    "        base_model.layers[i]._name += '_head'\n",
    "\n",
    "    # Last index of the head CNN\n",
    "    comb_out = base_model.layers[breakindex[flows-1]].output\n",
    "    #base_model.layers[breakindex[flows-1]]._name += '_lastcom'\n",
    "\n",
    "    seps = []\n",
    "    \n",
    "\n",
    "    for seg in range(classes):\n",
    "\n",
    "        res = SeparableConv2D(filters=16, kernel_size=3, \n",
    "                                strides=2, name=f'sepconv2d_d1_c{seg}')(comb_out)\n",
    "        res = ReLU(name=f'maxpool2d_d1_c{seg}')(res)\n",
    "        res = SeparableConv2D(filters=8, kernel_size=3, strides=2, name=f'sepconv2d_d2_c{seg}')(res)\n",
    "        res = ReLU(name=f'maxpool2d_d2_c{seg}')(res)\n",
    "\n",
    "        res = Flatten(name='flatten_'+str(seg))(res)\n",
    "        res = Dense(1)(res)\n",
    "        seps.append(res)\n",
    "        \n",
    "    out = Concatenate()(seps)\n",
    "    out = Activation('sigmoid' if loss=='binary_crossentropy' else 'softmax')(out)    \n",
    "    model = tf.keras.Model(input_img, out)\n",
    "\n",
    "    print('Params', model.count_params())\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=loss,\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),\n",
    "                           tf.keras.metrics.BinaryAccuracy(),\n",
    "                           tf.keras.metrics.Precision(),\n",
    "                           tf.keras.metrics.Recall(),\n",
    "                           tf.keras.metrics.TopKCategoricalAccuracy(),\n",
    "                           tf.keras.metrics.AUC(num_thresholds=200,\n",
    "                                                multi_label=True),\n",
    "                           tf.keras.metrics.TruePositives(),\n",
    "                           tf.keras.metrics.FalsePositives(),\n",
    "                           ],\n",
    "                  )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params 235222\n"
     ]
    }
   ],
   "source": [
    "model = Xception_block(X_train.shape[1:], 6,\n",
    "                       flows=6,\n",
    "                       loss='binary_crossentropy')\n",
    "\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 22:46:30.852602: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.5426 - categorical_accuracy: 0.1651 - binary_accuracy: 0.8024 - precision: 0.1745 - recall: 0.0498 - top_k_categorical_accuracy: 0.8629 - auc: 0.4998 - true_positives: 48.0000 - false_positives: 227.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 22:47:08.582044: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 46s 1s/step - loss: 0.5426 - categorical_accuracy: 0.1651 - binary_accuracy: 0.8024 - precision: 0.1745 - recall: 0.0498 - top_k_categorical_accuracy: 0.8629 - auc: 0.4998 - true_positives: 48.0000 - false_positives: 227.0000 - val_loss: 0.4809 - val_categorical_accuracy: 0.1618 - val_binary_accuracy: 0.8333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_top_k_categorical_accuracy: 0.8050 - val_auc: 0.4979 - val_true_positives: 0.0000e+00 - val_false_positives: 0.0000e+00\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.4508 - categorical_accuracy: 0.2118 - binary_accuracy: 0.8333 - precision: 0.0000e+00 - recall: 0.0000e+00 - top_k_categorical_accuracy: 0.8598 - auc: 0.5338 - true_positives: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 0.4686 - val_categorical_accuracy: 0.1826 - val_binary_accuracy: 0.8333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_top_k_categorical_accuracy: 0.8465 - val_auc: 0.5189 - val_true_positives: 0.0000e+00 - val_false_positives: 0.0000e+00\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 31s 988ms/step - loss: 0.4398 - categorical_accuracy: 0.2482 - binary_accuracy: 0.8333 - precision: 0.0000e+00 - recall: 0.0000e+00 - top_k_categorical_accuracy: 0.9055 - auc: 0.6064 - true_positives: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 0.4681 - val_categorical_accuracy: 0.1369 - val_binary_accuracy: 0.8333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_top_k_categorical_accuracy: 0.8548 - val_auc: 0.5491 - val_true_positives: 0.0000e+00 - val_false_positives: 0.0000e+00\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 30s 978ms/step - loss: 0.4253 - categorical_accuracy: 0.3011 - binary_accuracy: 0.8335 - precision: 1.0000 - recall: 0.0010 - top_k_categorical_accuracy: 0.9460 - auc: 0.6838 - true_positives: 1.0000 - false_positives: 0.0000e+00 - val_loss: 0.4799 - val_categorical_accuracy: 0.1992 - val_binary_accuracy: 0.8333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_top_k_categorical_accuracy: 0.8216 - val_auc: 0.5306 - val_true_positives: 0.0000e+00 - val_false_positives: 0.0000e+00\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 30s 952ms/step - loss: 0.4020 - categorical_accuracy: 0.3790 - binary_accuracy: 0.8349 - precision: 0.9091 - recall: 0.0104 - top_k_categorical_accuracy: 0.9574 - auc: 0.7378 - true_positives: 10.0000 - false_positives: 1.0000 - val_loss: 0.5189 - val_categorical_accuracy: 0.1369 - val_binary_accuracy: 0.8237 - val_precision: 0.1818 - val_recall: 0.0166 - val_top_k_categorical_accuracy: 0.8257 - val_auc: 0.5318 - val_true_positives: 4.0000 - val_false_positives: 18.0000\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 30s 977ms/step - loss: 0.3706 - categorical_accuracy: 0.5130 - binary_accuracy: 0.8394 - precision: 0.7397 - recall: 0.0561 - top_k_categorical_accuracy: 0.9813 - auc: 0.7987 - true_positives: 54.0000 - false_positives: 19.0000 - val_loss: 0.5301 - val_categorical_accuracy: 0.2324 - val_binary_accuracy: 0.8313 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_top_k_categorical_accuracy: 0.8133 - val_auc: 0.5461 - val_true_positives: 0.0000e+00 - val_false_positives: 3.0000\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 30s 959ms/step - loss: 0.3393 - categorical_accuracy: 0.5774 - binary_accuracy: 0.8470 - precision: 0.7516 - recall: 0.1225 - top_k_categorical_accuracy: 0.9875 - auc: 0.8467 - true_positives: 118.0000 - false_positives: 39.0000 - val_loss: 0.5188 - val_categorical_accuracy: 0.1577 - val_binary_accuracy: 0.8313 - val_precision: 0.2000 - val_recall: 0.0041 - val_top_k_categorical_accuracy: 0.8548 - val_auc: 0.5400 - val_true_positives: 1.0000 - val_false_positives: 4.0000\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 30s 955ms/step - loss: 0.3067 - categorical_accuracy: 0.6833 - binary_accuracy: 0.8612 - precision: 0.7993 - recall: 0.2233 - top_k_categorical_accuracy: 0.9948 - auc: 0.8863 - true_positives: 215.0000 - false_positives: 54.0000 - val_loss: 0.6705 - val_categorical_accuracy: 0.1784 - val_binary_accuracy: 0.8313 - val_precision: 0.2000 - val_recall: 0.0041 - val_top_k_categorical_accuracy: 0.8382 - val_auc: 0.5140 - val_true_positives: 1.0000 - val_false_positives: 4.0000\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 30s 955ms/step - loss: 0.2677 - categorical_accuracy: 0.7715 - binary_accuracy: 0.8787 - precision: 0.8503 - recall: 0.3302 - top_k_categorical_accuracy: 0.9958 - auc: 0.9318 - true_positives: 318.0000 - false_positives: 56.0000 - val_loss: 0.5928 - val_categorical_accuracy: 0.1701 - val_binary_accuracy: 0.8264 - val_precision: 0.0833 - val_recall: 0.0041 - val_top_k_categorical_accuracy: 0.8465 - val_auc: 0.5776 - val_true_positives: 1.0000 - val_false_positives: 11.0000\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 30s 964ms/step - loss: 0.2054 - categorical_accuracy: 0.8588 - binary_accuracy: 0.9143 - precision: 0.9134 - recall: 0.5369 - top_k_categorical_accuracy: 0.9990 - auc: 0.9613 - true_positives: 517.0000 - false_positives: 49.0000 - val_loss: 0.5307 - val_categorical_accuracy: 0.2158 - val_binary_accuracy: 0.8160 - val_precision: 0.2340 - val_recall: 0.0456 - val_top_k_categorical_accuracy: 0.9129 - val_auc: 0.6354 - val_true_positives: 11.0000 - val_false_positives: 36.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(X_train,to_categorical(y_train) , validation_data=(X_test, to_categorical(y_test)), \n",
    "                    epochs=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0910852971f271316cedd65fb9a6fba74ce8d38f423ea6cfef93e6992f0711c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
